{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2441d0be-cbb1-4fba-bbf0-fcb4ef4d3df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from huggingface_hub import login\n",
    "import pm4py\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "login(token=\"YOURTOKENHERE\", add_to_git_credential=True)\n",
    "\n",
    "def secure_mkdir(directory_path):\n",
    "    if not os.path.isdir(directory_path):\n",
    "        os.mkdir(directory_path)\n",
    "\n",
    "def secure_listdir(path, rm_dirs=[\".ipynb_checkpoints\", ]):\n",
    "    path_list = os.listdir(path)\n",
    "    for rm_dir in rm_dirs:\n",
    "        if rm_dir in path_list:\n",
    "            path_list.remove(rm_dir)\n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f32201-b83e-4853-8460-5b761cfa9412",
   "metadata": {},
   "source": [
    "# Calculate Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d8d32c",
   "metadata": {},
   "source": [
    "## Generate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07e8b7e-edea-4eae-8351-07f49507f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    [\n",
    "        \"Meta-Llama-3-8B-bpi13_closed_problem-199900595\",\n",
    "        \"Meta-Llama-3-8B-bpi13_closed_problem-534895718\",\n",
    "        \"Meta-Llama-3-8B-bpi13_closed_problem-787846414\",\n",
    "        \"Meta-Llama-3-8B-bpi13_closed_problem-862061404\",\n",
    "        \"Meta-Llama-3-8B-bpi13_closed_problem-996406378\"\n",
    "    ],\n",
    "        [\n",
    "        \"Meta-Llama-3-8B-bpi13_incidents-199900595\",\n",
    "        \"Meta-Llama-3-8B-bpi13_incidents-534895718\",\n",
    "        \"Meta-Llama-3-8B-bpi13_incidents-787846414\",\n",
    "        \"Meta-Llama-3-8B-bpi13_incidents-862061404\",\n",
    "        \"Meta-Llama-3-8B-bpi13_incidents-996406378\"\n",
    "    ],\n",
    "    [\n",
    "        \"Meta-Llama-3-8B-sepsis_cases-199900595\",\n",
    "        \"Meta-Llama-3-8B-sepsis_cases-534895718\",\n",
    "        \"Meta-Llama-3-8B-sepsis_cases-787846414\",\n",
    "        \"Meta-Llama-3-8B-sepsis_cases-862061404\",\n",
    "        \"Meta-Llama-3-8B-sepsis_cases-996406378\"\n",
    "    ],\n",
    "    [\n",
    "        \"Meta-Llama-3-8B-helpdesk-199900595\",\n",
    "        \"Meta-Llama-3-8B-helpdesk-534895718\",\n",
    "        \"Meta-Llama-3-8B-helpdesk-787846414\",\n",
    "        \"Meta-Llama-3-8B-helpdesk-862061404\",\n",
    "        \"Meta-Llama-3-8B-helpdesk-996406378\"\n",
    "    ],\n",
    "    [\n",
    "        \"Meta-Llama-3-8B-bpi12-199900595\",\n",
    "        \"Meta-Llama-3-8B-bpi12-534895718\",\n",
    "        \"Meta-Llama-3-8B-bpi12-787846414\",\n",
    "        \"Meta-Llama-3-8B-bpi12-862061404\",\n",
    "        \"Meta-Llama-3-8B-bpi12-996406378\",\n",
    "    ]\n",
    "]\n",
    "results_path = \"results\"\n",
    "results_csv_path = os.path.join(results_path, \"results.csv\")\n",
    "logs_results_colums = [\"model\", \"log\", \"fold\", \"fold_index\", \"case\", \"concept_name_pred\", \"concept_name_true\", \"similar\", ]\n",
    "logs_results = []\n",
    "for model_names in models:\n",
    "    all_res = []\n",
    "    for model_name in model_names:\n",
    "        \n",
    "        target_true_path = os.path.join(results_path, f'True_{model_name}.xes')\n",
    "        target_predict_path = os.path.join(results_path, f'Pred_{model_name}.xes')\n",
    "        log_true = pm4py.read_xes(target_true_path)\n",
    "        log_pred = pm4py.read_xes(target_predict_path)\n",
    "        \n",
    "        log_true = log_true[[\"case:concept:name\", \"concept:name\",]].reset_index().rename(columns={\"case:concept:name\": \"case\", \"concept:name\": \"concept_name_true\", \"index\": \"fold_index\"}).copy()\n",
    "        log_true[\"case\"] = log_true[\"case\"].astype(str)\n",
    "        log_true[\"concept_name_true\"] = log_true[\"concept_name_true\"].fillna(\"\")\n",
    "        log_true[\"concept_name_true\"] = log_true[\"concept_name_true\"].astype(str).str.strip()\n",
    "        \n",
    "        log_pred = log_pred[[\"case:concept:name\", \"concept:name\",]].reset_index().rename(columns={\"case:concept:name\": \"case\", \"concept:name\": \"concept_name_pred\", \"index\": \"fold_index\"}).fillna(\"\").copy()\n",
    "        log_pred[\"case\"] = log_pred[\"case\"].astype(str)\n",
    "        log_pred[\"concept_name_pred\"] = log_pred[\"concept_name_pred\"].fillna(\"\")\n",
    "        log_pred[\"concept_name_pred\"] = log_pred[\"concept_name_pred\"].astype(str).str.strip()\n",
    "        \n",
    "        log_merged = log_true.merge(log_pred, on=[\"case\", \"fold_index\"]).copy()\n",
    "        assert len(log_merged) == len(log_true) and len(log_merged) == len(log_pred)\n",
    "        log_merged[\"similar\"] = log_merged[\"concept_name_true\"] == log_merged[\"concept_name_pred\"]\n",
    "        model_info = model_name.split(\"-\")\n",
    "        log_merged[[\"model\", \"log\", \"fold\"]] =  \"-\".join(model_info[:-2]), model_info[-2], model_info[-1]\n",
    "        log_merged = log_merged[logs_results_colums].copy()\n",
    "        \n",
    "        logs_results.append(log_merged.copy())\n",
    "logs_results = pd.concat(logs_results).reset_index(drop=True)\n",
    "logs_results.to_csv(results_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a75ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_csv_path = os.path.join(results_path, \"results.csv\")\n",
    "results = pd.read_csv(results_csv_path, low_memory=False)\n",
    "results_by_fold = results[[\"model\", \"log\", \"fold\", \"similar\",]].groupby([\"model\", \"log\", \"fold\",]).agg(lambda x: round(x.mean(), 4)).reset_index().rename(columns={\"similar\": \"accuracy\"})\n",
    "display(results_by_fold)\n",
    "results_by_log = results_by_fold[[\"model\", \"log\", \"accuracy\",]].groupby([\"model\", \"log\",]).agg(lambda x: round(x.mean(), 3)).reset_index()\n",
    "display(results_by_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccaec01",
   "metadata": {},
   "source": [
    "## Investigate Hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1535f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_csv_path = os.path.join(results_path, \"results.csv\")\n",
    "results = pd.read_csv(results_csv_path, low_memory=False)\n",
    "concept_names = results[\"concept_name_true\"].unique()\n",
    "res_false = results[results[\"similar\"] == False].copy()\n",
    "hallucinations = res_false[res_false[\"concept_name_pred\"].apply(lambda x: x not in concept_names)].copy()\n",
    "display(hallucinations)\n",
    "nans = hallucinations[hallucinations[\"concept_name_pred\"].isna()].copy()\n",
    "nans = nans[[\"model\", \"log\", \"similar\"]].groupby([\"model\", \"log\",]).agg(\"count\")\n",
    "display(nans)\n",
    "display(nans.apply(lambda x: round(x / 5, 4), axis=0))\n",
    "hallucinations = hallucinations.dropna()[[\"model\", \"log\", \"similar\"]].groupby([\"model\", \"log\",]).agg(\"count\")\n",
    "display(hallucinations)\n",
    "display(hallucinations.apply(lambda x: round(x / 5, 4), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7f3ff9",
   "metadata": {},
   "source": [
    "## F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f5b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_csv_path = os.path.join(results_path, \"results.csv\")\n",
    "results = pd.read_csv(results_csv_path, low_memory=False)\n",
    "concept_names = results[\"concept_name_true\"].unique()\n",
    "false_results = results[results[\"similar\"] == False].copy()\n",
    "ind = false_results[false_results[\"concept_name_pred\"].apply(lambda x: x not in concept_names)].index\n",
    "results[\"concept_name_pred\"][results.apply(lambda x: x[\"concept_name_pred\"] not in concept_names and x[\"log\"] == \"sepsis_cases\" and x[\"concept_name_true\"] != \"ER Triage\", axis=1)] = \"ER Triage\" # set as existing, but wrong concept name\n",
    "results[\"concept_name_pred\"][results.apply(lambda x: x[\"concept_name_pred\"] not in concept_names and x[\"log\"] == \"sepsis_cases\" and x[\"concept_name_true\"] != \"CRP\", axis=1)] = \"CRP\" # set as existing, but wrong concept name\n",
    "results.loc[1860, \"concept_name_pred\"] = \"Completed\"\n",
    "results.loc[[88405, 93425, 95076, ], \"concept_name_pred\"] = \"Closed\"\n",
    "results.loc[101338, \"concept_name_pred\"] = \"Closed\"\n",
    "results.loc[108678, \"concept_name_pred\"] = \"Require upgrade\"\n",
    "results.loc[281677, \"concept_name_pred\"] = \"A_DECLINED\"\n",
    "display(results.loc[ind])\n",
    "false_results = results[results[\"similar\"] == False].copy()\n",
    "assert len(false_results[false_results[\"concept_name_pred\"].apply(lambda x: x not in concept_names)]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da5da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_by_fold = results[[\"model\", \"log\", \"fold\", \"concept_name_true\", \"concept_name_pred\", ]].groupby([\"model\", \"log\", \"fold\",]).agg(list).reset_index().rename(columns={\"similar\": \"accuracy\"})\n",
    "results_by_fold[\"f1_score\"] = results_by_fold.apply(lambda x: round(f1_score(x[\"concept_name_true\"], x[\"concept_name_pred\"], average=\"weighted\"), 4), axis=1)\n",
    "display(results_by_fold)\n",
    "results_by_log = results_by_fold[[\"model\", \"log\", \"f1_score\",]].groupby([\"model\", \"log\",]).agg(lambda x: round(x.mean(), 3)).reset_index()\n",
    "display(results_by_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161d3f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_by_fold = results[[\"model\", \"log\", \"fold\", \"concept_name_true\", \"concept_name_pred\", ]].groupby([\"model\", \"log\", \"fold\",]).agg(list).reset_index().rename(columns={\"similar\": \"accuracy\"})\n",
    "results_by_fold[\"f1_score\"] = results_by_fold.apply(lambda x: round(f1_score(x[\"concept_name_true\"], x[\"concept_name_pred\"], average=\"macro\"), 4), axis=1)\n",
    "display(results_by_fold)\n",
    "results_by_log = results_by_fold[[\"model\", \"log\", \"f1_score\",]].groupby([\"model\", \"log\",]).agg(lambda x: round(x.mean(), 3)).reset_index()\n",
    "display(results_by_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    [\n",
    "        \"Meta-Llama-3-8B-bpi13_closed_problem-199900595\",\n",
    "        \"Meta-Llama-3-8B-bpi13_closed_problem-534895718\",\n",
    "        \"Meta-Llama-3-8B-bpi13_closed_problem-787846414\",\n",
    "        \"Meta-Llama-3-8B-bpi13_closed_problem-862061404\",\n",
    "        \"Meta-Llama-3-8B-bpi13_closed_problem-996406378\"\n",
    "    ],\n",
    "        [\n",
    "        \"Meta-Llama-3-8B-bpi13_incidents-199900595\",\n",
    "        \"Meta-Llama-3-8B-bpi13_incidents-534895718\",\n",
    "        \"Meta-Llama-3-8B-bpi13_incidents-787846414\",\n",
    "        \"Meta-Llama-3-8B-bpi13_incidents-862061404\",\n",
    "        \"Meta-Llama-3-8B-bpi13_incidents-996406378\"\n",
    "    ],\n",
    "    [\n",
    "        \"Meta-Llama-3-8B-sepsis_cases-199900595\",\n",
    "        \"Meta-Llama-3-8B-sepsis_cases-534895718\",\n",
    "        \"Meta-Llama-3-8B-sepsis_cases-787846414\",\n",
    "        \"Meta-Llama-3-8B-sepsis_cases-862061404\",\n",
    "        \"Meta-Llama-3-8B-sepsis_cases-996406378\"\n",
    "    ],\n",
    "    [\n",
    "        \"Meta-Llama-3-8B-helpdesk-199900595\",\n",
    "        \"Meta-Llama-3-8B-helpdesk-534895718\",\n",
    "        \"Meta-Llama-3-8B-helpdesk-787846414\",\n",
    "        \"Meta-Llama-3-8B-helpdesk-862061404\",\n",
    "        \"Meta-Llama-3-8B-helpdesk-996406378\"\n",
    "    ],\n",
    "    [\n",
    "        \"Meta-Llama-3-8B-bpi12-199900595\",\n",
    "        \"Meta-Llama-3-8B-bpi12-534895718\",\n",
    "        \"Meta-Llama-3-8B-bpi12-787846414\",\n",
    "        \"Meta-Llama-3-8B-bpi12-862061404\",\n",
    "        \"Meta-Llama-3-8B-bpi12-996406378\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import statistics\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, set_seed\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "import argparse\n",
    "import pm4py\n",
    "login(token=\"YOURTOKENHERE\", add_to_git_credential=True)\n",
    "\n",
    "def prepare_sample_text(example, tokenizer, remove_indent=False, start=None, end=None, pred=False):\n",
    "    \"\"\"Prepare the text from a sample of the dataset.\"\"\"\n",
    "    thread = example[\"event_list\"]\n",
    "    if start != None and end != None:\n",
    "        thread = thread[start:end]\n",
    "    text = \"\"\n",
    "    thread = thread[-20:]\n",
    "    for message in thread:\n",
    "        text += f\"{message}{tokenizer.eos_token}\\n\"\n",
    "    return text\n",
    "\n",
    "test_input_ids_all = []\n",
    "for model_names in models:\n",
    "    for model_name in model_names:\n",
    "        model_info = model_name.split(\"-\")\n",
    "        model = \"-\".join(model_info[:-2])\n",
    "        dataset_name =  model_info[-2]\n",
    "        random_seed = int(model_info[-1])\n",
    "        set_seed(random_seed)\n",
    "        model_path = \"skaltenp/\" + model_name\n",
    "\n",
    "        dataset_path = \"skaltenp/\" + dataset_name\n",
    "        model_name = model_path.split(\"/\")[-1]\n",
    "\n",
    "        dataset = load_dataset(dataset_path)\n",
    "        train_data = dataset[\"train\"].train_test_split(train_size=0.8, shuffle=True, seed=random_seed)\n",
    "        test_data = train_data[\"test\"]\n",
    "        train_data = train_data[\"train\"].train_test_split(train_size=0.8, shuffle=True, seed=random_seed)\n",
    "        valid_data = train_data[\"test\"]\n",
    "        train_data = train_data[\"train\"]\n",
    "        dataset = DatasetDict(\n",
    "            {\n",
    "                \"train\": train_data,\n",
    "                \"valid\": valid_data,\n",
    "                \"test\": test_data\n",
    "            }\n",
    "        )\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path, \n",
    "            use_fast=True, \n",
    "        )\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        tokenizer.truncation_side = \"left\"\n",
    "\n",
    "        test_len = len(dataset[\"test\"])\n",
    "        test_input_ids = []\n",
    "        for example in dataset[\"test\"]:\n",
    "            res = prepare_sample_text(example, tokenizer)\n",
    "            with torch.no_grad():\n",
    "                inputs = tokenizer(\n",
    "                    res, \n",
    "                    return_tensors=\"pt\", \n",
    "                    #max_length=4096, \n",
    "                    #truncation=True\n",
    "                ).to(\"cpu\")\n",
    "                test_input_ids.append(len(inputs[\"input_ids\"][0]))\n",
    "        test_input_ids_all.append([model, dataset_name, random_seed, test_input_ids])\n",
    "print(test_input_ids_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97841617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from matplotlib import pyplot as plt\n",
    "for i in test_input_ids_all[:1]:\n",
    "    px.box(i[-1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5031a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids_all_new = pd.DataFrame(test_input_ids_all, columns=[\"model\", \"dataset_name\", \"random_seed\", \"test_input_ids\"])\n",
    "results = test_input_ids_all_new.copy()\n",
    "def boxpl(x):\n",
    "    x = np.array(x[\"test_input_ids\"])\n",
    "    return [min(x), np.quantile(x, 0.25), np.quantile(x, 0.5), np.quantile(x, 0.75), max(x)]\n",
    "results[[\"min\", \"q25\", \"median\", \"q75\", \"max\"]] = results.apply(boxpl, axis=1, result_type=\"expand\")\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8c6c8b-62fc-4015-af39-c156e925fb82",
   "metadata": {},
   "source": [
    "# Calculate Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b7648-a811-4a41-b0b9-4a7c54b3e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    [\n",
    "        \"Meta-Llama-3-8B-bpi13_closed_problem-199900595\",\n",
    "        \"Meta-Llama-3-8B-bpi13_closed_problem-534895718\",\n",
    "        \"Meta-Llama-3-8B-bpi13_closed_problem-787846414\",\n",
    "        \"Meta-Llama-3-8B-bpi13_closed_problem-862061404\",\n",
    "        \"Meta-Llama-3-8B-bpi13_closed_problem-996406378\"\n",
    "    ],\n",
    "    [\n",
    "        \"Meta-Llama-3-8B-bpi13_incidents-199900595\",\n",
    "        \"Meta-Llama-3-8B-bpi13_incidents-534895718\",\n",
    "        \"Meta-Llama-3-8B-bpi13_incidents-787846414\",\n",
    "        \"Meta-Llama-3-8B-bpi13_incidents-862061404\",\n",
    "        \"Meta-Llama-3-8B-bpi13_incidents-996406378\"\n",
    "    ],\n",
    "    [\n",
    "        \"Meta-Llama-3-8B-sepsis_cases-199900595\",\n",
    "        \"Meta-Llama-3-8B-sepsis_cases-534895718\",\n",
    "        \"Meta-Llama-3-8B-sepsis_cases-787846414\",\n",
    "        \"Meta-Llama-3-8B-sepsis_cases-862061404\",\n",
    "        \"Meta-Llama-3-8B-sepsis_cases-996406378\"\n",
    "    ],\n",
    "    [\n",
    "        \"Meta-Llama-3-8B-helpdesk-199900595\",\n",
    "        \"Meta-Llama-3-8B-helpdesk-534895718\",\n",
    "        \"Meta-Llama-3-8B-helpdesk-787846414\",\n",
    "        \"Meta-Llama-3-8B-helpdesk-862061404\",\n",
    "        \"Meta-Llama-3-8B-helpdesk-996406378\"\n",
    "    ],\n",
    "    [\n",
    "        \"Meta-Llama-3-8B-bpi12-199900595\",\n",
    "        \"Meta-Llama-3-8B-bpi12-534895718\",\n",
    "        \"Meta-Llama-3-8B-bpi12-787846414\",\n",
    "        \"Meta-Llama-3-8B-bpi12-862061404\",\n",
    "        \"Meta-Llama-3-8B-bpi12-996406378\"\n",
    "    ]\n",
    "]\n",
    "results_path = \"results\"\n",
    "errors = []\n",
    "errors_names = [\"Model\", \"Dataset\", \"Total errors\",]\n",
    "for model_names in models:\n",
    "    for model_name in model_names:\n",
    "        row = [model_name, model_name.split(\"-\")[4], ]\n",
    "        target_true_path = os.path.join(results_path, f'True_{model_name}.xes')\n",
    "        target_predict_path = os.path.join(results_path, f'Pred_{model_name}.xes')\n",
    "        xes_content = \"\"\n",
    "        with open(target_predict_path, \"r\", encoding=\"utf-8\") as read_file:\n",
    "            xes_content = read_file.read()\n",
    "        xes_content = xes_content.replace(\n",
    "            \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n",
    "<!-- This file has been generated with the OpenXES library. It conforms -->\n",
    "<!-- to the XML serialization of the XES standard for log storage and -->\n",
    "<!-- management. -->\n",
    "<!-- XES standard version: 1.0 -->\n",
    "<!-- OpenXES library version: 1.0RC7 -->\n",
    "<!-- OpenXES is available from http://www.openxes.org/ -->\"\"\",\n",
    "            \"\"\n",
    "        )\n",
    "        row.append(len(re.findall(r\"<!-- .* -->\", xes_content)))\n",
    "        #print(re.findall(r\"<!-- .* -->\", xes_content))\n",
    "        errors.append(row)\n",
    "errors = pd.DataFrame(errors, columns=errors_names)\n",
    "display(errors.drop(columns=\"Model\").groupby(\"Dataset\").agg(\"mean\").reset_index())\n",
    "errors.drop(columns=\"Model\").groupby(\"Dataset\").agg(\"mean\").reset_index().to_csv(\"errors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c79169-a65b-40d4-bad1-ed5463ef8b88",
   "metadata": {},
   "source": [
    "# Calculate time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1b6801-beaf-484e-bcfd-e45fe18c9194",
   "metadata": {},
   "source": [
    "We calculate the time by hand using the logs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
